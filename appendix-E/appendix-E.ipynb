{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a67fc3",
   "metadata": {},
   "source": [
    "## Appendix E: Parameter-efficient Finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b308b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.8\n",
      "numpy version: 2.2.6\n",
      "tiktoken version: 0.12.0\n",
      "torch version: 2.9.1\n",
      "tensorflow version: 2.20.0\n",
      "pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # For OpenAI's pretrained weights\n",
    "        \"pandas\"      # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2e48f",
   "metadata": {},
   "source": [
    "### E.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec040972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "# import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch06 import (\n",
    "#     download_and_unzip_spam_data,\n",
    "#     create_balanced_dataset,\n",
    "#     random_split\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "# The book originally used\n",
    "# except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "# in the code above.\n",
    "# However, some VPN users reported issues with `urllib`, so the code was updated\n",
    "# to use `requests` instead\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99da942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f3692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe905759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a60d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451ee1b",
   "metadata": {},
   "source": [
    "### E.3 Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdc794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 51.3kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.42MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 76.7kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:15<00:00, 6.60MiB/s]  \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 3.12MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 633kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 641kiB/s] \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf19ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81929ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d2a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac23d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616546e3",
   "metadata": {},
   "source": [
    "### E.4 Parameter-efficient finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008b2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "    def forward(self, x):\n",
    "        x = (self.alpha / self.rank) * (x @ self.A @ self.B)\n",
    "        return x;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9413b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "027cd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af21f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00244b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5954ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "011cd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89882c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.346, Val loss 0.325\n",
      "Ep 1 (Step 000100): Train loss 0.063, Val loss 0.144\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Ep 2 (Step 000150): Train loss 0.054, Val loss 0.045\n",
      "Ep 2 (Step 000200): Train loss 0.058, Val loss 0.122\n",
      "Ep 2 (Step 000250): Train loss 0.041, Val loss 0.199\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.020, Val loss 0.153\n",
      "Ep 3 (Step 000350): Train loss 0.017, Val loss 0.189\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.017, Val loss 0.094\n",
      "Ep 4 (Step 000450): Train loss 0.001, Val loss 0.135\n",
      "Ep 4 (Step 000500): Train loss 0.005, Val loss 0.278\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.015, Val loss 0.185\n",
      "Ep 5 (Step 000600): Train loss 0.003, Val loss 0.112\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Training completed in 4.11 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40d0c2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch06 import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12090d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.62%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 97.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
