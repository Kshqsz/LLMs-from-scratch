{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7892e46",
   "metadata": {},
   "source": [
    "### 7.2 Preparing a dataset for supervised instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0d8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# The book originally used the following code below\n",
    "# However, urllib uses older protocol settings that\n",
    "# can cause problems for some readers using a VPN.\n",
    "# The `requests` version above is more robust\n",
    "# in that regard.\n",
    "\n",
    "\"\"\"\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf775cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5671c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropariately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99986fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropariately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n"
     ]
    }
   ],
   "source": [
    "print(format_input(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0fc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropariately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n"
     ]
    }
   ],
   "source": [
    "print(format_input(data[999]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c784a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropariately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a58826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ea4e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ba6345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103710ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2cc35",
   "metadata": {},
   "source": [
    "### 7.3 Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63894d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8952257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb83ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    device = \"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb0a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "006e765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "custom_collate_draft_1(batch)\n",
    "print(custom_collate_draft_1(batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680ae5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    device = \"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d8cd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     1,     2,     3,     4],\n",
       "         [    5,     6, 50256, 50256, 50256],\n",
       "         [    7,     8,     9, 50256, 50256]]),\n",
       " tensor([[    1,     2,     3,     4, 50256],\n",
       "         [    6, 50256, 50256, 50256, 50256],\n",
       "         [    8,     9, 50256, 50256, 50256]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_collate_draft_2(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673f8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    ignore_index = -100,\n",
    "    allowed_max_length = None,\n",
    "    device = \"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        \n",
    "        mask = targets == pad_token_id\n",
    "        indics = torch.nonzero(mask).squeeze()\n",
    "\n",
    "        if indics.numel() > 1:\n",
    "            targets[indics[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba50912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     1,     2,     3,     4],\n",
       "         [    5,     6, 50256, 50256, 50256],\n",
       "         [    7,     8,     9, 50256, 50256]]),\n",
       " tensor([[    1,     2,     3,     4, 50256],\n",
       "         [    6, 50256,  -100,  -100,  -100],\n",
       "         [    8,     9, 50256,  -100,  -100]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_collate_fn(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40d689",
   "metadata": {},
   "source": [
    "### 7.4 Creating data loaders for an instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21ddb33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2bd9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device = device,\n",
    "    allowed_max_length = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c278d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6807b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "babd2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2734289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13175209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "          257,  2882,   326,  1331,  1845,    72,  1286, 32543,   262,  2581,\n",
       "           13,   198,   198, 21017, 46486,    25,   198, 30003,  6525,   262,\n",
       "         6827,  1262,   257,   985,   576,    13,   198,   198, 21017, 23412,\n",
       "           25,   198,   464,  5156,   318,   845, 13779,    13,   198,   198,\n",
       "        21017, 18261,    25,   198,   464,  5156,   318,   355, 13779,   355,\n",
       "          257,  4936,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "        50256, 50256], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7202ffce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
       "         2882,   326,  1331,  1845,    72,  1286, 32543,   262,  2581,    13,\n",
       "          198,   198, 21017, 46486,    25,   198, 30003,  6525,   262,  6827,\n",
       "         1262,   257,   985,   576,    13,   198,   198, 21017, 23412,    25,\n",
       "          198,   464,  5156,   318,   845, 13779,    13,   198,   198, 21017,\n",
       "        18261,    25,   198,   464,  5156,   318,   355, 13779,   355,   257,\n",
       "         4936,    13, 50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ef0d3",
   "metadata": {},
   "source": [
    "### 7.5 Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81c674bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c2ec8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropariately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4cd6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens = 35,\n",
    "    context_size = BASE_CONFIG[\"context_length\"],\n",
    "    eos_id = 50256\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "377ce9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropariately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ded24250",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = (generated_text[len(input_text):].replace(\"### Response:\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "190c1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d6444",
   "metadata": {},
   "source": [
    "### 7.6 Finetuning the LLM on instruction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fd73522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1689e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9338, device='mps:0')\n",
      "tensor(3.8665, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batchs = 5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batchs = 5)\n",
    "\n",
    "print(train_loss)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da92c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Convert the active sentence to passive: 'The chef cooks the meal every day.'\",\n",
       " 'input': '',\n",
       " 'output': 'The meal is cooked by the chef every day.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1922b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a040bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.657, Val loss 2.642\n",
      "Ep 1 (Step 000005): Train loss 1.130, Val loss 1.061\n",
      "Ep 1 (Step 000010): Train loss 0.827, Val loss 0.897\n",
      "Ep 1 (Step 000015): Train loss 0.811, Val loss 0.866\n",
      "Ep 1 (Step 000020): Train loss 0.738, Val loss 0.837\n",
      "Ep 1 (Step 000025): Train loss 0.713, Val loss 0.811\n",
      "Ep 1 (Step 000030): Train loss 0.755, Val loss 0.790\n",
      "Ep 1 (Step 000035): Train loss 0.682, Val loss 0.771\n",
      "Ep 1 (Step 000040): Train loss 0.636, Val loss 0.763\n",
      "Ep 1 (Step 000045): Train loss 0.601, Val loss 0.751\n",
      "Ep 1 (Step 000050): Train loss 0.634, Val loss 0.744\n",
      "Ep 1 (Step 000055): Train loss 0.723, Val loss 0.725\n",
      "Ep 1 (Step 000060): Train loss 0.686, Val loss 0.707\n",
      "Ep 1 (Step 000065): Train loss 0.620, Val loss 0.699\n",
      "Ep 1 (Step 000070): Train loss 0.504, Val loss 0.695\n",
      "Ep 1 (Step 000075): Train loss 0.540, Val loss 0.699\n",
      "Ep 1 (Step 000080): Train loss 0.572, Val loss 0.692\n",
      "Ep 1 (Step 000085): Train loss 0.486, Val loss 0.674\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5,\n",
    "    start_context = format_input(val_data[0]), tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"time : {exeuction_time_minutes}mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1e640",
   "metadata": {},
   "source": [
    "### 7.7 Extracting and saving responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4026bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model = model,\n",
    "        idx = text_to_token_ids(input_text, tokenizer).to(device)\n",
    "        max_new_tokens = 256,\n",
    "        context_size = BASE_CONFIG[\"context_length\"],\n",
    "        eos_id = 50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
